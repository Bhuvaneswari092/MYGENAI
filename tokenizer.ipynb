{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1jhcbj2_OUre"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "texts=[\n",
        "    \"i love deep learning\",\n",
        "    \"RNNs are powerful for sequence data\",\n",
        "    \"Tokenization is the first step\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=Tokenizer()\n",
        "tokenizer.fit_on_texts(texts)"
      ],
      "metadata": {
        "id": "suaMHg6wPmuX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences=tokenizer.texts_to_sequences(texts)"
      ],
      "metadata": {
        "id": "qqXd6Ro8QMhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "padded_sequences=pad_sequences(sequences,padding='post')"
      ],
      "metadata": {
        "id": "tm-xmm5dQVJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"word Index:\\n\",tokenizer.word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ma9Qm94eQ0Aj",
        "outputId": "4255de3c-271b-499e-b0f1-552e86293d74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "word Index:\n",
            " {'i': 1, 'love': 2, 'deep': 3, 'learning': 4, 'rnns': 5, 'are': 6, 'powerful': 7, 'for': 8, 'sequence': 9, 'data': 10, 'tokenization': 11, 'is': 12, 'the': 13, 'first': 14, 'step': 15}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"sequences:\\n\",sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gKt_1r0VRDfu",
        "outputId": "05e72b26-cd23-4101-947e-70aaa3bec9aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sequences:\n",
            " [[1, 2, 3, 4], [5, 6, 7, 8, 9, 10], [11, 12, 13, 14, 15]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"paddedsequences:\\n\",padded_sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2SLyBNKxRPo-",
        "outputId": "f83a48bb-a064-43cc-a7cf-b0f71249be22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "paddedsequences:\n",
            " [[ 1  2  3  4  0  0]\n",
            " [ 5  6  7  8  9 10]\n",
            " [11 12 13 14 15  0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"This is a sample sentence for tokenizer demo.\"\n",
        "word_tokens=text.split()\n",
        "print(\"Word Tokens:\", word_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1-clJMU-Qr-",
        "outputId": "c51a7353-fa0f-4943-d339-c90d1a3a0a32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokens: ['This', 'is', 'a', 'sample', 'sentence', 'for', 'tokenizer', 'demo.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "char_tokens=list(text)\n",
        "print(\"Character Tokens:\",char_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_DTd3eI2_cyY",
        "outputId": "8bdc76f4-088b-48ec-a770-9b750d67abca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Character Tokens: ['T', 'h', 'i', 's', ' ', 'i', 's', ' ', 'a', ' ', 's', 'a', 'm', 'p', 'l', 'e', ' ', 's', 'e', 'n', 't', 'e', 'n', 'c', 'e', ' ', 'f', 'o', 'r', ' ', 't', 'o', 'k', 'e', 'n', 'i', 'z', 'e', 'r', ' ', 'd', 'e', 'm', 'o', '.']\n"
          ]
        }
      ]
    }
  ]
}